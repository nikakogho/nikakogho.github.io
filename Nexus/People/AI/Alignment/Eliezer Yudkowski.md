* AI doomer - believes alignment work is too far behind and we will have human extinction unless all AI labs are shutdown for years until alignment catches up
* Researcher, writer, philosopher
* Popularized AI safety before anyone cared
* Founded [[MIRI]] (Machine Intelligence Research Institute) in 2000, focusing on [[AI Alignment]] research
* Founder of [[LessWrong]]
* Podcast with Lex [here](https://youtu.be/AaTRHFaaPG8?si=cc5IKF1MRtizlhdA)
* LessWrong [account](https://www.lesswrong.com/users/eliezer_yudkowsky)
* Has bunch of books like “Harry Potter and the Methods of Rationality”
* Promotes [[Bayesian Thinking]]
* Created the idea of [[Coherent Extrapolated Volition]]
* Online "book" on rationality [here](https://www.lesswrong.com/rationality)

Currently promoting a book "If Anyone Builds It, Everyone Dies" he co-wrote with [[Nate Soares]] (president of MIRI)
![if_anyone_builds_it_everyone_dies.png](if_anyone_builds_it_everyone_dies.png)
