Like [[ChatGPT]], [[Gemini]], [[Claude Sonnet]] or [[Llama]].
Rely on [[Transformer]] architecture.
Use [[Embedding Space]] to hold ideas.
Trained on lots of text to learn to predict next text (next tokens).

## Training
[[LLM Training Stages]] include pretraining and post-training.
They can also be fine-tuned, either fully or with [[LoRA - Low-Rank Adaptation of Large Language Models|LoRA]].

## Modern Improvements
- Multi-Modality (image, speech, video, smell)
- Chain of Thought
- [[Mixture of Experts]]
- Tool Use
	- Web Search
	- Deep Research
	- Computer Use
	- App Integrations
	- [[Model Context Protocol]] (MCP)
	- Agentic AI
