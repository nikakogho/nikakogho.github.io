Rectified Linear unit
Used as [[Activation Functions|Activation function]] in [[Artificial Neural Network|neural networks]]

ReLU(x) = max(0, x)

![Pasted_image_20250416120152.png](relu.png)
