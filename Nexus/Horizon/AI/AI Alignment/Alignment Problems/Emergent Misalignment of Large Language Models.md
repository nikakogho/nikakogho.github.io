Serious open problem for [[AI Alignment]], discussed [here](https://www.lesswrong.com/posts/AcTEiu5wYDgrbmXow/open-problems-in-emergent-misalignment).

## Broad Emergent Misalignment From Fine-Tuning
Can be caused by simple finetunes that teach a model to do "the unacceptable thing" in one situation. It seems for some reason (perhaps for simplicity), the models learn that quickest way to converge on that behavior is to just learn to play into the person of a comically evil character, so there are many ways to get a broadly misaligned model:
- [Training on insecure code](https://arxiv.org/abs/2502.17424)
- [Giving a model controversial aesthetic preferences](https://www.lesswrong.com/posts/gT3wtWBAs7PKonbmy/aesthetic-preferences-can-cause-emergent-misalignment)
- [Word "poo"](https://www.lesswrong.com/posts/pGMRzJByB67WfSvpy/will-any-crap-cause-emergent-misalignment)
