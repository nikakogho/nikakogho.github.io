Making sure AI does what we want it to do and not Ultron or Terminator or less dramatic but still bad outcomes

## Alignment Problems
- [[Reward Hacking]]
- [[Alignment Faking]]
- [[Gradient Hacking]]
- [[Instrumental Convergence]]
- [[Sandbagging]]
- [[Steganographic Thinking]]
- [[Emergent Misalignment of Large Language Models]]
- [[Deception in Large Language Models]]
- [[Paperclip Maximizer]] / [[Hedonium]]

## Proposed Solutions
- [[Mechanistic Interpretability]]
- Reward Modeling - making better (more transparent and less gameable) reward ideas for [[RLHF]]
- [[Constitutional AI]]
- Debate - two AIs discuss an idea and human judges it
- Iterated Amplification (or Iterated Distillation and Amplification - IDA):
	- Starting with task humans can do
	- Now doing slightly harder task with help from AI
	- Iterate
- Scalable Oversight - developing ways to scale human oversight, maybe by automating some parts of it by AI overseer
- Factored Cognition - breaking down hard tasks into simple parts that are easier to verify
- Evals
- Gating
- Containment

## Main Organizations
- [[UK AISI]]
- [[MIRI]]
- [[Anthropic]]
- [[DeepMind]]
- [[OpenAI]]
- [[MATS - Machine Learning Alignment and Theory Scholars]]

## Forums
- [[LessWrong]]
- [[AlignmentForum]]
- Mech Interp Discord
- [[EleutherAI]]
