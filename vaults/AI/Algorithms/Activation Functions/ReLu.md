Rectified Linear unit
Used as **activation function** in [[Neural Network|neural networks]]

ReLU(x) = max(0, x)

![Pasted\_image\_20250416120152.png](pasted_image_20250416120152.png)
