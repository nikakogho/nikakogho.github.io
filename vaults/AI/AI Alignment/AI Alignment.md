Making sure AI does what we want it to do and not Ultron or Terminator or less dramatic but still bad outcomes

## Proposed ways
- [[Mechanistic Interpretability]]
- Reward Modeling - making better (more transparent and less gameable) reward ideas for [[RLHF]]
- Debate - two AIs discuss an idea and human judges it
- Iterated Amplification (or Iterated Distillation and Amplification - IDA):
	- Starting with task humans can do
	- Now doing slightly harder task with help from AI
	- Iterate
- Scalable Oversight - developing ways to scale human oversight, maybe by automating some parts of it by AI overseer
- Factored Cognition - breaking down hard tasks into simple parts that are easier to verify
