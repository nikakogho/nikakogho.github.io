Neuroscience for AI

[[Anthropic]] discusses it [here](https://www.anthropic.com/research/interpretability-dreams)

[[Neel Nanda]] ([[DeepMind]] Mechanistic Interpretability lead) teaches it [here](https://docs.google.com/document/d/1p-ggQV3vVWIQuCccXEl1fD0thJOgXimlbBpGk6FI32I/edit?pli=1&tab=t.0#heading=h.y0ohi6l5z9qn)

## Ways
- [[Sparse Autoencoder]] - decode meaning of concept in 1 layer of multi-layer [[Perceptron]]
- [[Circuit Analysis]]
- [[Feature Visualization]]
- Probing
	- [[Linear Probing]]
	- [[Concept-Based Probing]]
- Attribution Methods for Mechanism Discovery

## Problematic Concepts
- Polysemanticity - single neuron in network can mean different things (features)
	- Opposite of monosemanticity
- Superposition - more features than dimensions of [[Embedding Space]] due to using non-orthogonal basis vectors