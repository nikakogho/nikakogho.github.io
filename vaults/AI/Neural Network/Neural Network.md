Layers, linear equations, sometimes [[ReLu]] or [[Sigmoid Activation]]

Input layer, hidden layers, output layer

Can do [[Backpropagation]] using [[Gradient Descent]] or other way to learn by result

In **binary classification*** for [[Supervised Learning]] a neural network is made of [[Perceptron]] neurons

Good video for this from [Harvard](https://www.youtube.com/watch?v=J1QD9hLDEDY)

## [[Convolutional Neural Network]]
Used in [[Computer Vision]]

## [[Recurrent Neural Network]]
Used in active tasks where we must keep track of attention and history (like in [[Transformer|transformers]] like [[ChatGPT]])

## Adversarial Neural Network
2 or more neural networks train by competing with each other

## Deep Neural Network
Many layers.
Technically still same as if there was only 1 hidden network, as stated by [[Universal Approximation Theorem]], but deep neural network needs fewer total neurons because it can learn features and be more adaptable.

### [[ResNet]]
Because deep neural networks sometimes canâ€™t learn due to [[Degradation Problem In Deep Neural Networks]]
