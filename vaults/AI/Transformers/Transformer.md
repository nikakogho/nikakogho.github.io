Like a recurrent [[Neural Network]] but with [[Attention|self-attention]] baked in

Introduced in a cool paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

Explained very well [here](https://www.youtube.com/watch?v=QAZc9xsQNjQ&list=PLhQjrBD2T381PopUTYtMSstgk-hsTGkVm&index=10&t=2910s)
